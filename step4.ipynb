{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization [xgboost](https://github.com/dmlc/xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the options there're for tuning?\n",
    "* [GridSearch](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "* [RandomizedSearch](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html)\n",
    "\n",
    "All right!\n",
    "Xgboost has about 20 params:\n",
    "1. base_score\n",
    "2. **colsample_bylevel**\n",
    "3. **colsample_bytree** \n",
    "4. **gamma**\n",
    "5. **learning_rate**\n",
    "6. **max_delta_step**\n",
    "7. **max_depth**\n",
    "8. **min_child_weight**\n",
    "9. missing\n",
    "10. **n_estimators**\n",
    "11. nthread\n",
    "12. **objective**\n",
    "13. **reg_alpha**\n",
    "14. **reg_lambda**\n",
    "15. **scale_pos_weight**\n",
    "16. **seed**\n",
    "17. silent\n",
    "18. **subsample**\n",
    "\n",
    "\n",
    "\n",
    "Let's for tuning will be use 12 of them them with 5-10 possible values, so... there're 12^5 - 12^10 possible cases.\n",
    "If you will check one case in 10s, for **12^5** you need **30 days** for **12^10** about **20K** years :). \n",
    "\n",
    "This is too long.. but there's a thid option - **Bayesan optimisation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('bike.csv')\n",
    "train['datetime'] = pd.to_datetime( train['datetime'] )\n",
    "train['day'] = train['datetime'].map(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assing_test_samples(data, last_training_day=0.3, seed=1):\n",
    "    days = data.day.unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(days)\n",
    "    test_days = days[: int(len(days) * 0.3)]\n",
    "    \n",
    "    data['is_test'] = data.day.isin(test_days)\n",
    "\n",
    "\n",
    "def select_features(data):\n",
    "    columns = data.columns[ (data.dtypes == np.int64) | (data.dtypes == np.float64) | (data.dtypes == np.bool) ].values    \n",
    "    return [feat for feat in columns if feat not in ['count', 'casual', 'registered'] and 'log' not in feat ] \n",
    "\n",
    "def get_X_y(data, target_variable):\n",
    "    features = select_features(data)\n",
    "        \n",
    "    X = data[features].values\n",
    "    y = data[target_variable].values\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def train_test_split(train, target_variable):\n",
    "    df_train = train[train.is_test == False]\n",
    "    df_test  = train[train.is_test == True]\n",
    "    \n",
    "    X_train, y_train = get_X_y(df_train, target_variable)\n",
    "    X_test, y_test = get_X_y(df_test, target_variable)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_predict(train, model, target_variable):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target_variable)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return (y_test, y_pred)\n",
    "\n",
    "def post_pred(y_pred):\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    return y_pred\n",
    "\n",
    "def rmsle(y_true, y_pred, y_pred_only_positive=True):\n",
    "    if y_pred_only_positive: y_pred = post_pred(y_pred)\n",
    "        \n",
    "    diff = np.log(y_pred+1) - np.log(y_true+1)\n",
    "    mean_error = np.square(diff).mean()\n",
    "    return np.sqrt(mean_error)\n",
    "\n",
    "assing_test_samples(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def etl_datetime(df):\n",
    "    df['year'] = df['datetime'].map(lambda x: x.year)\n",
    "    df['month'] = df['datetime'].map(lambda x: x.month)\n",
    "\n",
    "    df['hour'] = df['datetime'].map(lambda x: x.hour)\n",
    "    df['minute'] = df['datetime'].map(lambda x: x.minute)\n",
    "    df['dayofweek'] = df['datetime'].map(lambda x: x.dayofweek)\n",
    "    df['weekend'] = df['datetime'].map(lambda x: x.dayofweek in [5,6])\n",
    "\n",
    "    \n",
    "etl_datetime(train)\n",
    "\n",
    "train['{0}_log'.format('count')] = train['count'].map(lambda x: np.log2(x) )\n",
    "\n",
    "for name in ['registered', 'casual']:\n",
    "    train['{0}_log'.format(name)] = train[name].map(lambda x: np.log2(x+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparmeters using Bayesian optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.327769943579\n",
      "SCORE: 0.402119793524\n",
      "SCORE: 0.441702998659\n",
      "SCORE: 0.344952075056\n",
      "SCORE: 0.332483052772\n",
      "SCORE: 0.415230694098\n",
      "SCORE: 0.326159133525\n",
      "SCORE: 0.366755440868\n",
      "SCORE: 0.336209948966\n",
      "SCORE: 0.320813982928\n",
      "SCORE: 0.33925039026\n",
      "SCORE: 0.363387131966\n",
      "SCORE: 0.324682064912\n",
      "SCORE: 0.382678760754\n",
      "SCORE: 0.488176057958\n",
      "{'x_learning_rate': 0.0803514512536536, 'x_reg_alpha': 0.44303008763740737, 'n_estimators': 421.0, 'x_max_depth': 17.0, 'x_subsample': 0.9561807797584932, 'x_colsample_bytree': 0.8214374064161822}\n"
     ]
    }
   ],
   "source": [
    "def objective(space):\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth = space['max_depth'],\n",
    "        n_estimators = int(space['n_estimators']),\n",
    "        subsample = space['subsample'],\n",
    "        colsample_bytree = space['colsample_bytree'],\n",
    "        learning_rate = space['learning_rate'],\n",
    "        reg_alpha = space['reg_alpha']\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, 'count')\n",
    "    eval_set  = [( X_train, y_train), ( X_test, y_test)]\n",
    "\n",
    "\n",
    "    (_, registered_pred) = fit_and_predict(train, model, 'registered_log')\n",
    "    (_, casual_pred) = fit_and_predict(train, model, 'casual_log')\n",
    "   \n",
    "    y_test = train[train.is_test == True]['count']\n",
    "    y_pred = (np.exp2(registered_pred) - 1) + (np.exp2(casual_pred) -1)\n",
    "    \n",
    "    score = rmsle(y_test, y_pred)\n",
    "    print \"SCORE:\", score\n",
    "\n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "space ={\n",
    "    'max_depth': hp.quniform(\"x_max_depth\", 2, 20, 1),\n",
    "    'n_estimators': hp.quniform(\"n_estimators\", 100, 1000, 1),\n",
    "    'subsample': hp.uniform ('x_subsample', 0.8, 1), \n",
    "    'colsample_bytree': hp.uniform ('x_colsample_bytree', 0.1, 1), \n",
    "    'learning_rate': hp.uniform ('x_learning_rate', 0.01, 0.1), \n",
    "    'reg_alpha': hp.uniform ('x_reg_alpha', 0.1, 1)\n",
    "}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=15,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "1. http://hyperopt.github.io/hyperopt/\n",
    "2. https://districtdatalabs.silvrback.com/parameter-tuning-with-hyperopt\n",
    "3. http://fastml.com/optimizing-hyperparams-with-hyperopt/\n",
    "4. https://github.com/Far0n/xgbfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
